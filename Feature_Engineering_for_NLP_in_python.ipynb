{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Engineering for NLP in python.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNzvTXNiRMStvjM9QP6CyjZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sha863/MSc-Dissertation-2022/blob/main/Feature_Engineering_for_NLP_in_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chap1 Basic Feature Extraction"
      ],
      "metadata": {
        "id": "mUqqvEeisrF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Number of characters"
      ],
      "metadata": {
        "id": "tE5wwUmgt3yn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQaGLhwisljA"
      },
      "outputs": [],
      "source": [
        "# Compute the number of characters\n",
        "text = \"I don't know.\"\n",
        "num_char = len(text)\n",
        "\n",
        "# Print the number of characters\n",
        "print(num_char)\n",
        "\n",
        "# Create a 'num_chars' feature\n",
        "df['num_chars'] = df['review'].apply(len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Number of words"
      ],
      "metadata": {
        "id": "B7ePd_yNuDmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that returns number of words in string\n",
        "def word_count(string):\n",
        "# Split the string into words\n",
        "words = string.split()\n",
        "\n",
        "# Return length of words list\n",
        "return len(words)\n",
        "\n",
        "# Create num_words feature in df\n",
        "df['num_words'] = df['review'].apply(word_count)"
      ],
      "metadata": {
        "id": "yoOAghYUuGiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Average Word Length"
      ],
      "metadata": {
        "id": "RlKiX-Yaua8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function that returns average word length\n",
        "def avg_word_length(x):\n",
        "\n",
        "# Split the string into words\n",
        "words = x.split()\n",
        "\n",
        "# Compute length of each word and store in a separate list\n",
        "word_lengths = [len(word) for word in words]\n",
        "\n",
        "# Compute average word length\n",
        "avg_word_length = sum(word_lengths)/len(words)\n",
        "\n",
        "# Return average word length\n",
        "return(avg_word_length)\n",
        "\n",
        "# Create a new feature avg_word_length\n",
        "df['avg_word_length'] = df['review'].apply(avg_word_length)"
      ],
      "metadata": {
        "id": "YEP0OVw3ulZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hashtags and mentions"
      ],
      "metadata": {
        "id": "CNuIdr_ivB4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that returns number of hashtags\n",
        "def hashtag_count(string):\n",
        "\n",
        "# Split the string into words\n",
        "words = string.split()\n",
        "\n",
        "# Create a list of hashtags\n",
        "hashtags = [word for word in words if word.startswith('#')]\n",
        "\n",
        "# Return number of hashtags\n",
        "return len(hashtags)"
      ],
      "metadata": {
        "id": "ldHippXnvJgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Readability Test\n",
        "\n",
        "* Flesch reading ease\n",
        "* Gunning fog index"
      ],
      "metadata": {
        "id": "LJ4YHa_evimx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Textatistic class\n",
        "from textatistic import Textatistic\n",
        "\n",
        "# Create a Textatistic Object\n",
        "readability_scores = Textatistic(text).scores\n",
        "\n",
        "# Generate scores\n",
        "print(readability_scores['flesch_score'])\n",
        "print(readability_scores['gunningfog_score'])"
      ],
      "metadata": {
        "id": "UJHjMD-uvl4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 2 Text Preprocessing"
      ],
      "metadata": {
        "id": "jYpotcWCv4FH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization using spaCy"
      ],
      "metadata": {
        "id": "JjlAln8Rv63b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# Initiliaze string\n",
        "string = \"Hello! I don't know what I'm doing here.\"\n",
        "# Create a Doc object\n",
        "doc = nlp(string)\n",
        "# Generate list of tokens\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHShbwX3woNw",
        "outputId": "c447dc24-17cc-4799-9574-55c6b35e9cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', 'I', 'do', \"n't\", 'know', 'what', 'I', \"'m\", 'doing', 'here', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization using spaCy"
      ],
      "metadata": {
        "id": "GBCxMc39wukj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# Initiliaze string\n",
        "string = \"Hello! I don't know what I'm doing here.\"\n",
        "# Create a Doc object\n",
        "doc = nlp(string)\n",
        "# Generate list of lemmas\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "print(lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3uSiiIFwwFN",
        "outputId": "ccc46233-9f5e-4795-ac36-e61430ed69fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', '!', '-PRON-', 'do', 'not', 'know', 'what', '-PRON-', 'be', 'do', 'here', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Cleaning\n",
        "\n",
        "* Unnecessary whitespaces and escape sequences\n",
        "* Punctuations\n",
        "* Special characters (numbers, emojis, etc.)\n",
        "* Stopwords"
      ],
      "metadata": {
        "id": "2hbfpRo8w8-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing non-alphabetic characters"
      ],
      "metadata": {
        "id": "C6j2O8bzxJrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string = \"\"\"\n",
        "OMG!!!! This is like\n",
        "the best thing ever \\t\\n.\n",
        "Wow, such an amazing song! I'm hooked. Top 5 definitely. ?\n",
        "\"\"\"\n",
        "import spacy\n",
        "# Generate list of tokens\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(string)\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "...\n",
        "...\n",
        "# Remove tokens that are not alphabetic\n",
        "a_lemmas = [lemma for lemma in lemmas\n",
        "if lemma.isalpha() or lemma == '-PRON-']\n",
        "# Print string after text cleaning\n",
        "print(' '.join(a_lemmas))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIkbdOoJxrTu",
        "outputId": "47a4c324-5b83-41b1-c016-8f59a57b500d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OMG this be like the good thing ever wow such an amazing song -PRON- be hooked top definitely\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing stopwords using spaCy"
      ],
      "metadata": {
        "id": "b0ECt4ynyEjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of stopwords\n",
        "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "string = \"\"\"\n",
        "OMG!!!! This is like\n",
        "the best thing ever \\t\\n.\n",
        "Wow, such an amazing song! I'm hooked. Top 5 definitely. ?\n",
        "\"\"\"\n",
        "...\n",
        "...\n",
        "# Remove stopwords and non-alphabetic tokens\n",
        "a_lemmas = [lemma for lemma in lemmas\n",
        "if lemma.isalpha() and lemma not in stopwords]\n",
        "# Print string after text cleaning\n",
        "print(' '.join(a_lemmas))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTvwiz37yGfZ",
        "outputId": "1300d439-3a24-48aa-8582-357acf2082ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OMG like good thing wow amazing song hooked definitely\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other text preprocessing techniques\n",
        "* Removing HTML/XML tags\n",
        "* Replacing accented characters (such as é)\n",
        "* Correcting spelling errors"
      ],
      "metadata": {
        "id": "OenG0FGzyXNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part of Speech Tagging"
      ],
      "metadata": {
        "id": "Aj__xkbpybjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POS tagging using spaCy"
      ],
      "metadata": {
        "id": "mfuIK2RcyjOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# Initiliaze string\n",
        "string = \"Jane is an amazing guitarist\"\n",
        "# Create a Doc object\n",
        "doc = nlp(string)\n",
        "...\n",
        "...\n",
        "# Generate list of tokens and pos tags\n",
        "pos = [(token.text, token.pos_) for token in doc]\n",
        "print(pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H61bLuUnyipd",
        "outputId": "f1822041-3358-4917-d552-69877647ef45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Jane', 'PROPN'), ('is', 'AUX'), ('an', 'DET'), ('amazing', 'ADJ'), ('guitarist', 'NOUN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Name entity Recoginition"
      ],
      "metadata": {
        "id": "sJMPGkaUy58i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "string = \"John Doe is a software engineer working at Google. He lives in France.\"\n",
        "\n",
        "# Load model and create Doc object\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(string)\n",
        "\n",
        "# Generate named entities\n",
        "ne = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(ne)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4-wB-exzBWt",
        "outputId": "3878811e-fb36-4fe2-c926-e19dd55ac7d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('John Doe', 'PERSON'), ('Google', 'ORG'), ('France', 'GPE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chap 3 Bag of Words Model"
      ],
      "metadata": {
        "id": "TlzouPwJzVMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BOW using SKLEARN"
      ],
      "metadata": {
        "id": "KJ0hllIkzhIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "corpus = pd.Series([\n",
        "'The lion is the king of the jungle',\n",
        "'Lions have lifespans of a decade',\n",
        "'The lion is an endangered species'\n",
        "])\n",
        "\n",
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Create CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "# Generate matrix of word vectors\n",
        "bow_matrix = vectorizer.fit_transform(corpus)\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyEaiF3kzvIF",
        "outputId": "48a7ad5a-51b4-44fc-da24-2f3662e33793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 1 1 1 0 1 0 1 0 3]\n",
            " [0 1 0 1 0 0 0 1 0 1 1 0 0]\n",
            " [1 0 1 0 1 0 0 0 1 0 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a BoW Naive Bayes classifier"
      ],
      "metadata": {
        "id": "c2PZcnav0JG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create CountVectorizer object\n",
        "vectorizer = CountVectorizer(strip_accents='ascii', stop_words='english', lowercase=False)\n",
        "\n",
        "# Import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.25)\n",
        "\n",
        "# Generate training Bow vectors\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Generate test BoW vectors\n",
        "X_test_bow = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "lkyUZgjpz_yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Naive Bayes classifier"
      ],
      "metadata": {
        "id": "EIFrSeUd0dEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import MultinomialNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Create MultinomialNB object\n",
        "clf = MultinomialNB()\n",
        "\n",
        "# Train clf\n",
        "clf.fit(X_train_bow, y_train)\n",
        "\n",
        "# Compute accuracy on test set\n",
        "accuracy = clf.score(X_test_bow, y_test)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "BnxBmmr_0em4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building n-gram models"
      ],
      "metadata": {
        "id": "b7tdej8U0uRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates only bigrams.\n",
        "bigrams = CountVectorizer(ngram_range=(2,2))\n",
        "\n",
        "# Generates unigrams, bigrams and trigrams.\n",
        "ngrams = CountVectorizer(ngram_range=(1,3))"
      ],
      "metadata": {
        "id": "FhEOgYZd00yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chap 4 Building tf-idf document vectors"
      ],
      "metadata": {
        "id": "6id7zY6_1Uhp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tf-idf using scikit-learn"
      ],
      "metadata": {
        "id": "R6nHRTl31cIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Create TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "# Generate matrix of word vectors\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "print(tfidf_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfqBEIUG1uzH",
        "outputId": "e46ec256-5369-46b4-e812-a4141bd186fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.         0.25434658 0.33443519\n",
            "  0.33443519 0.         0.25434658 0.         0.25434658 0.\n",
            "  0.76303975]\n",
            " [0.         0.46735098 0.         0.46735098 0.         0.\n",
            "  0.         0.46735098 0.         0.46735098 0.35543247 0.\n",
            "  0.        ]\n",
            " [0.45954803 0.         0.45954803 0.         0.34949812 0.\n",
            "  0.         0.         0.34949812 0.         0.         0.45954803\n",
            "  0.34949812]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine similarity"
      ],
      "metadata": {
        "id": "9Nu7bkJa12_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the cosine_similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Define two 3-dimensional vectors A and B\n",
        "A = (4,7,1)\n",
        "B = (5,2,3)\n",
        "# Compute the cosine score of A and B\n",
        "score = cosine_similarity([A], [B])\n",
        "# Print the cosine score\n",
        "print(score)"
      ],
      "metadata": {
        "id": "MX_BEMTZ12uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Generate matrix of tf-idf vectors\n",
        "tfidf_matrix = vectorizer.fit_transform(movie_plots)\n",
        "\n",
        "# Import cosine_similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Generate cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "get_recommendations('The Lion King', cosine_sim, indices)"
      ],
      "metadata": {
        "id": "lPQj0qQk2Wpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beyond n-grams: word embeddings\n",
        "\n",
        "* Mapping words into an n-dimensional vector space\n",
        "* Produced using deep learning and huge amounts of data\n",
        "* Discern how similar two words are to each other\n",
        "* Used to detect synonyms and antonyms\n",
        "* Captures complex relationships\n",
        "King - Queen → Man - Woman\n",
        "France - Paris → Russia - Moscow\n",
        "* Dependent on spacy model; independent of dataset you use"
      ],
      "metadata": {
        "id": "P4laHdC62nPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word embeddings using spaCy"
      ],
      "metadata": {
        "id": "w5PHA2Oc3C6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load model and create Doc object\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp('I am happy')\n",
        "# Generate word vectors for each token\n",
        "for token in doc:\n",
        " print(token.vector)\n",
        "doc = nlp(\"happy joyous sad\")\n",
        "for token1 in doc:\n",
        " for token2 in doc:\n",
        "  print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "metadata": {
        "id": "f8b_jmMc2mbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Review\n",
        "\n",
        "* Basic features (characters, words, mentions, etc.)\n",
        "* Readability scores\n",
        "* Tokenization and lemmatization\n",
        "* Text cleaning\n",
        "* Part-of-speech tagging & named entity recognition\n",
        "* n-gram modeling\n",
        "* tf-idf\n",
        "* Cosine similarity\n",
        "* Word embeddings"
      ],
      "metadata": {
        "id": "BsRF999s3v_o"
      }
    }
  ]
}